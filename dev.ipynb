{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import stanza\n",
    "import string\n",
    "from progress.bar import IncrementalBar\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 09:54:02 INFO: Loading these models for language: ro (Romanian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | rrt     |\n",
      "=======================\n",
      "\n",
      "2022-05-13 09:54:03 INFO: Use device: cpu\n",
      "2022-05-13 09:54:03 INFO: Loading: tokenize\n",
      "2022-05-13 09:54:03 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "LABEL_MAPPER = {\n",
    "    'PF': 1,\n",
    "    'PF_reprezentat': 1,\n",
    "    'PF_delegat': 1,\n",
    "    'PJ': 3,\n",
    "    'PJ_reprezentat': 3,\n",
    "    'PJ_delegat': 3,\n",
    "    'STAT': 5,\n",
    "    'STAT_reprezentat': 5,\n",
    "    'STAT_delegat': 5,\n",
    "    'Locatie_PJ': 7\n",
    "}\n",
    "ROM_SPECIAL_CHARS = 'ǎǍăĂâÂȃȂșȘîÎțȚãÃȋȊ' \n",
    "PUNCTUATION_SPECIAL_CHARS = '„”“«»’‘…´″‚'\n",
    "MONEY_CHARS = '€$£'\n",
    "\n",
    "ALLOWED_CHARS = string.printable + ROM_SPECIAL_CHARS + PUNCTUATION_SPECIAL_CHARS + MONEY_CHARS\n",
    "\n",
    "nlp = stanza.Pipeline(lang='ro', processors='tokenize', tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dataset = []\n",
    "\n",
    "# for nume_fisier in ['bogdan', 'ioana', 'madalina', 'marius', 'sergiu']:\n",
    "#     with open(f'dosare/{nume_fisier}.json') as json_file:\n",
    "#         json_var = json.load(json_file)\n",
    "#         json_dataset.extend(json_var)\n",
    "\n",
    "for nume_fisier in os.listdir(os.path.join('dosare','new')):\n",
    "    with open(os.path.join('dosare','new',nume_fisier)) as json_file:\n",
    "        json_doc = json.load(json_file)\n",
    "        json_dataset.extend(json_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32752"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_exemple_incorecte = []\n",
    "for json_entry in json_dataset:\n",
    "    text = json_entry['data']['ner']\n",
    "\n",
    "    annotations = json_entry['annotations'][-1]['result'] # -> list (UPDATE: O singura adnotare per exemplu)\n",
    "\n",
    "    if annotations['value']['end'] != len(text)-1:\n",
    "        lista_exemple_incorecte.append(text)\n",
    "\n",
    "    # sorted_annotations = sorted(annotations, key=lambda x: x['value']['start'])\n",
    "\n",
    "    # current_idx = 0\n",
    "    # for annotation in sorted_annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SC FITOCOM SRL TÂRGU MURES\n"
     ]
    }
   ],
   "source": [
    "output_json = []\n",
    "id_idx = 0\n",
    "\n",
    "with IncrementalBar('Tokenizing...', max=len(json_dataset)) as bar:\n",
    "    for json_entry in json_dataset[0:5]:\n",
    "        sentence_dict = {'id': id_idx, 'ner_tags': [], 'ner_ids': [], 'tokens': [], 'space_after': []}\n",
    "\n",
    "        text = json_entry['data']['ner']\n",
    "\n",
    "        annotations = json_entry['annotations'][-1]['result'] # -> list (UPDATE: O singura adnotare per exemplu)\n",
    "        sorted_annotations = sorted(annotations, key=lambda x: x['value']['start'])\n",
    "\n",
    "        current_idx = 0\n",
    "        for annotation in sorted_annotations:\n",
    "            annot = annotation['value']\n",
    "\n",
    "            start_idx = annot['start']\n",
    "            if current_idx < start_idx:\n",
    "                current_text_fragment = text[current_idx:start_idx]\n",
    "                doc = nlp(current_text_fragment)\n",
    "                for sentence in doc.sentences:\n",
    "                    for token in sentence.tokens:\n",
    "                        not_allowed_chars = set(token.text).difference(ALLOWED_CHARS)\n",
    "                        if not not_allowed_chars:\n",
    "                            sentence_dict['tokens'].append(token.text)\n",
    "                            sentence_dict['ner_tags'].append('O')\n",
    "                            sentence_dict['ner_ids'].append(0)\n",
    "                            sentence_dict['space_after'].append(True if (current_idx+token.end_char<len(text) and text[current_idx+token.end_char] == ' ') else False)\n",
    "                        else:\n",
    "                            print('Am gasit caractere dubioase')\n",
    "\n",
    "            current_idx = start_idx\n",
    "                        \n",
    "            current_text_fragment = annot['text']\n",
    "            doc = nlp(current_text_fragment)\n",
    "\n",
    "            for sentence in doc.sentences:\n",
    "                flag_first_token = True # Used for special chars, if they are first in token, they will be skipped\n",
    "                for index, token in enumerate(sentence.tokens):\n",
    "                    not_allowed_chars = set(token.text).difference(ALLOWED_CHARS)\n",
    "                    if not not_allowed_chars:\n",
    "                        sentence_dict['tokens'].append(token.text)\n",
    "                        if index == 0 or flag_first_token:\n",
    "                            sentence_dict['ner_tags'].append('B-'+annot['labels'][0])\n",
    "                            sentence_dict['ner_ids'].append(LABEL_MAPPER[annot['labels'][0]])\n",
    "\n",
    "                            flag_first_token = False\n",
    "                        else:\n",
    "                            sentence_dict['ner_tags'].append('I-'+annot['labels'][0])\n",
    "                            sentence_dict['ner_ids'].append(LABEL_MAPPER[annot['labels'][0]]+1)\n",
    "                        sentence_dict['space_after'].append(True if (current_idx+token.end_char<len(text) and text[current_idx+token.end_char] == ' ') else False)\n",
    "                    elif token.text[0] == '\\ufeff' and token.text.replace('\\ufeff', ''):\n",
    "                        sentence_dict['tokens'].append(token.text.replace('\\ufeff', ''))\n",
    "                        if index == 0 or flag_first_token:\n",
    "                            sentence_dict['ner_tags'].append('B-'+annot['labels'][0])\n",
    "                            sentence_dict['ner_ids'].append(LABEL_MAPPER[annot['labels'][0]])\n",
    "\n",
    "                            flag_first_token = False\n",
    "                        else:\n",
    "                            sentence_dict['ner_tags'].append('I-'+annot['labels'][0])\n",
    "                            sentence_dict['ner_ids'].append(LABEL_MAPPER[annot['labels'][0]]+1)\n",
    "                        sentence_dict['space_after'].append(True if (current_idx+token.end_char<len(text) and text[current_idx+token.end_char] == ' ') else False)\n",
    "                    else:\n",
    "                        print('Am gasit caractere dubioase')\n",
    "\n",
    "            current_idx = annot['end']\n",
    "\n",
    "        current_text_fragment = text[current_idx:]\n",
    "        doc = nlp(current_text_fragment)\n",
    "\n",
    "        for sentence in doc.sentences:\n",
    "            for token in sentence.tokens:\n",
    "                not_allowed_chars = set(token.text).difference(ALLOWED_CHARS)\n",
    "                if not not_allowed_chars:\n",
    "                    sentence_dict['tokens'].append(token.text)\n",
    "                    sentence_dict['ner_tags'].append('O')\n",
    "                    sentence_dict['ner_ids'].append(0)\n",
    "                    sentence_dict['space_after'].append(True if (current_idx+token.end_char<len(text) and text[current_idx+token.end_char] == ' ') else False)\n",
    "                else:\n",
    "                    print('Am gasit caractere dubioase')\n",
    "\n",
    "        output_json.append(sentence_dict)\n",
    "\n",
    "        id_idx += 1\n",
    "\n",
    "        bar.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "900d496225abd65d7719b2faff7ed6c312794b3f81a8808bb1301fdf4739ee0d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ner_dosare_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
